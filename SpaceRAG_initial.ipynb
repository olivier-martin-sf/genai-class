{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanyashukla/genai-class/blob/main/SpaceRAG_initial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pao9QWqE7yLn"
      },
      "outputs": [],
      "source": [
        "IN_COLAB = False\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    import os\n",
        "    os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
        "    os.environ[\"TOGETHER_API_KEY\"] = userdata.get(\"TOGETHER_API_KEY\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJNpOy777yLn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "repo_url = \"https://github.com/wandb/weave.git\"\n",
        "target_folder = \"weave_cookbooks\"\n",
        "subdirectory = \"examples/cookbooks\"\n",
        "branch = \"anish/add-spacerag-example\"\n",
        "\n",
        "if not os.path.exists(target_folder) and IN_COLAB:\n",
        "    print(f\"Cloning repository: {repo_url}\")\n",
        "\n",
        "    # Clone the entire repository to a temporary folder\n",
        "    temp_folder = \"temp_weave_repo\"\n",
        "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \"--branch\", branch, repo_url, temp_folder], check=True)\n",
        "\n",
        "    # Move the desired subdirectory to the target folder\n",
        "    shutil.move(os.path.join(temp_folder, subdirectory), target_folder)\n",
        "\n",
        "    # Remove the temporary folder\n",
        "    shutil.rmtree(temp_folder)\n",
        "\n",
        "    print(f\"Successfully cloned {subdirectory} from branch '{branch}' to {target_folder}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Folder '{target_folder}' already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPU5GWX27yLo"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(target_folder) and IN_COLAB:\n",
        "    %cd weave_cookbooks/rag/spacerag\n",
        "    !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W38uw2oh7yLr"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "from weave import Evaluation\n",
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "from together import Together\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjF12eEJ7yLr"
      },
      "outputs": [],
      "source": [
        "weave.init('spacerag_nov4')\n",
        "\n",
        "# SERVE MODEL FROM TOGETHER ENDPOINT\n",
        "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGbs2ryx7yLs"
      },
      "outputs": [],
      "source": [
        "# CHUNK DATA FROM EXTERNAL KNOWLEDGEBASE\n",
        "@weave.op\n",
        "def get_chunked_data(file):\n",
        "    # get data - file\n",
        "    with open(file, 'r') as file:\n",
        "        # Read the contents of the file into a variable\n",
        "        text = file.read()\n",
        "\n",
        "    # split doc into chunks\n",
        "    chunk_size = 10000 # üõ†Ô∏èLEVER\n",
        "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)] # üõ†Ô∏èLEVER (advanced)\n",
        "    return chunks\n",
        "\n",
        "# EMBED DATA\n",
        "@weave.op\n",
        "def get_text_embedding(input):\n",
        "    api_key_openai = os.environ[\"OPENAI_API_KEY\"]\n",
        "    client = OpenAI(api_key=api_key_openai)\n",
        "\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\", # üõ†Ô∏èLEVER\n",
        "        input=input\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# MAKE VECTORDB\n",
        "@weave.op\n",
        "def make_vector_db(file):\n",
        "    # get chunked data from function get_chunked_data()\n",
        "    chunks = get_chunked_data(file)\n",
        "    # embed data\n",
        "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
        "    # embed data into vectordb\n",
        "    d = text_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(text_embeddings)\n",
        "    return index, chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRh3mP_L7yLs"
      },
      "outputs": [],
      "source": [
        "file = './data/space.txt'\n",
        "index, chunks = make_vector_db(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8RuuvYW7yLt"
      },
      "outputs": [],
      "source": [
        "# ANSWER QUESTION\n",
        "@weave.op\n",
        "def predict(model, prompt):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=1.0, # üõ†Ô∏èLEVER\n",
        "        top_k=1000, # üõ†Ô∏èLEVER\n",
        "        max_tokens=1024,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    answer = []\n",
        "    for chunk in completion:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            answer.append(chunk.choices[0].delta.content)\n",
        "\n",
        "    result = ''.join(answer)\n",
        "    print(result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mLorY2M7yLt"
      },
      "outputs": [],
      "source": [
        "# RETRIEVE CHUNKS SIMILAR TO THE QUESTION\n",
        "@weave.op\n",
        "def retrieve_context(question: str) -> list:\n",
        "    question_embeddings = np.array([get_text_embedding(question)])\n",
        "    # Retrieve similar chunks from the vectorDB\n",
        "    D, I = index.search(question_embeddings, k=1) # üõ†Ô∏èLEVER k=?\n",
        "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "    return retrieved_chunk\n",
        "\n",
        "class SpaceRAGModel(weave.Model):\n",
        "    model: str\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, question: str):\n",
        "        retrieved_chunk = retrieve_context(question)\n",
        "        print(\"Question: \"+question)\n",
        "\n",
        "        # Combine context and question in a prompt # üõ†Ô∏èLEVER\n",
        "        prompt = f\"\"\"\n",
        "        Use this context to answer the question, don't use any prior knowledge.\n",
        "        Be concise in your answers.\n",
        "        ---------------------\n",
        "        {retrieved_chunk}\n",
        "        ---------------------\n",
        "        Question: {question}\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        answer = predict(self.model, prompt)\n",
        "        print(\"___________________________\")\n",
        "        return {'answer': answer, 'retrieved_chunk': retrieved_chunk}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpO97J2T7yLt"
      },
      "outputs": [],
      "source": [
        "def string_to_dict(input_string):\n",
        "    # Use regular expressions to find all JSON-like objects in the string\n",
        "    json_objects = re.findall(r'\\{.*?\\}', input_string)\n",
        "\n",
        "    # Initialize an empty dictionary to store the combined results\n",
        "    combined_dict = {}\n",
        "\n",
        "    for obj in json_objects:\n",
        "        try:\n",
        "            # Parse each JSON object\n",
        "            parsed_dict = json.loads(obj)\n",
        "            # Update the combined dictionary with the parsed data\n",
        "            combined_dict.update(parsed_dict)\n",
        "        except (ValueError, json.JSONDecodeError) as e:\n",
        "            print(f\"Error processing part: {obj}\\nError: {e}\")\n",
        "\n",
        "    return combined_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWA1mI3R7yLu"
      },
      "outputs": [],
      "source": [
        "dataset_ref = weave.ref(\"weave:///lavanyashukla/spacedata/object/space_dataset_llm_comprehensive:VBd5Ys7b3hGFmJJqdGTATVQYgKKrg70EiNV5FdpwFxs\").get()\n",
        "small_questions = dataset_ref.rows[:5] # üîµ NOTE: CHANGE TO 5 WHEN RUNNING OUT YOUR EXPERIMENTS TO QUICKLY TEST.\n",
        "                                        # CHANGE BACK TO 50 BEFORE FINAL LEADERBOARD SUBMISSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ou5EOu07yLu"
      },
      "outputs": [],
      "source": [
        "def replace_nan_in_dict(result):\n",
        "    for key in result:\n",
        "        if isinstance(result[key], float) and np.isnan(result[key]):\n",
        "            result[key] = 0\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2K7y4jY7yLu"
      },
      "outputs": [],
      "source": [
        "# Evaluate with an LLM\n",
        "@weave.op\n",
        "def llm_judge_scorer(ground_truth: str, model_output: dict) -> dict:\n",
        "    scorer_llm = \"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\"\n",
        "    answer = model_output['answer']\n",
        "    retrieved_chunk = model_output['retrieved_chunk']\n",
        "\n",
        "    eval_rubrics = [\n",
        "    {\n",
        "        \"metric\": \"concise\",\n",
        "        \"rubrics\": \"\"\"\n",
        "        false: The answer is long and difficult to understand.\n",
        "        true: The answer is completely concise, readable and engaging.\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"metric\": \"relevant\",\n",
        "        \"rubrics\": \"\"\"\n",
        "        false: The answer is not relevant to the original text, or has significant flaws.\n",
        "        true: The answer is completely relevant to the original text, and provides additional value or insight.\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"metric\": \"ü•áaccurate\",\n",
        "        \"rubrics\": \"\"\"\n",
        "        Compare the factual content of the model's answer with the correct answer. Ignore any differences in style, grammar, or punctuation.\n",
        "        false: There is a disagreement between the model's answer and the correct answer.\n",
        "        true: The model's answer contains all the same details as the correct answer.\n",
        "        \"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "    scoring_prompt = \"\"\"\n",
        "    You have the correct answer, original text and the model's answer below.\n",
        "    Based on the specified evaluation metric and rubric, assign a true or false score the summary.\n",
        "    Then, return a JSON object with the metric name as the key and the evaluation score (false or true) as the value. Don't output anything else.\n",
        "\n",
        "    # Evaluation metric:\n",
        "    {metric}\n",
        "\n",
        "    # Evaluation rubrics:\n",
        "    {rubrics}\n",
        "\n",
        "    # Correct Answer\n",
        "    {ground_truth}\n",
        "\n",
        "    # Original Text\n",
        "    {retrieved_chunk}\n",
        "\n",
        "    # Model Answer\n",
        "    {model_answer}\n",
        "\n",
        "    \"\"\"\n",
        "    evals = \"\"\n",
        "    for i in eval_rubrics:\n",
        "        eval_output = predict(scorer_llm,\n",
        "            scoring_prompt.format(\n",
        "                ground_truth=ground_truth, retrieved_chunk=retrieved_chunk, model_answer=answer,\n",
        "                metric=i[\"metric\"], rubrics=i[\"rubrics\"]\n",
        "            ))+\" \"\n",
        "        evals+=eval_output\n",
        "    # evals_json = format_string_to_json(evals)\n",
        "    evals_dict = string_to_dict(evals)\n",
        "    # print(\"___________________________\")\n",
        "    # print(evals_dict)\n",
        "    # print(\"___________________________\")\n",
        "    return evals_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4cRBlJt7yLu"
      },
      "outputs": [],
      "source": [
        "# def ragas_score(question, ground_truth, model_output):\n",
        "#     from datasets import Dataset\n",
        "#     from ragas import evaluate\n",
        "#     from ragas.metrics import faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision\n",
        "\n",
        "#     metric_modules = [\n",
        "#         answer_relevancy,\n",
        "#         context_recall,\n",
        "#     ]\n",
        "\n",
        "#     # Convert the retrieved_chunk to a list of strings\n",
        "#     contexts = [str(chunk) for chunk in model_output[\"retrieved_chunk\"]]\n",
        "\n",
        "#     qa_dataset = Dataset.from_dict(\n",
        "#         {\n",
        "#             \"question\": [question],\n",
        "#             \"ground_truth\": [ground_truth],\n",
        "#             \"answer\": [model_output[\"answer\"]],\n",
        "#             \"contexts\": [contexts],  # Wrap contexts in another list\n",
        "#         }\n",
        "#     )\n",
        "#     result = evaluate(qa_dataset, metrics=metric_modules,\n",
        "#                       raise_exceptions=False)\n",
        "#     return replace_nan_in_dict(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoqWszGw7yLu"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def tonic_validate_score(question: str, ground_truth: str, model_output: dict) -> dict:\n",
        "    from tonic_validate import Benchmark, ValidateScorer\n",
        "    from tonic_validate.metrics import DuplicationMetric, AnswerSimilarityMetric, AnswerConsistencyMetric\n",
        "\n",
        "    metric_modules = [DuplicationMetric(), AnswerSimilarityMetric(), AnswerConsistencyMetric()]\n",
        "\n",
        "    def get_llm_response(question):\n",
        "        return {\n",
        "            \"llm_answer\": model_output['answer'],\n",
        "            \"llm_context_list\": (\n",
        "                [model_output['retrieved_chunk']]\n",
        "                if isinstance(model_output['retrieved_chunk'], str)\n",
        "                else model_output['retrieved_chunk']\n",
        "            ),\n",
        "        }\n",
        "\n",
        "    benchmark = Benchmark(questions=[question], answers=[ground_truth])\n",
        "    scorer = ValidateScorer(metrics=metric_modules)\n",
        "    run = scorer.score(benchmark, get_llm_response)\n",
        "    return run.run_data[0].scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LnvJEcy7yLu"
      },
      "outputs": [],
      "source": [
        "models = [\"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\",\n",
        "          \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "          \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
        "          ]  # üõ†Ô∏èLEVER\n",
        "for model in models:\n",
        "    rag_model = SpaceRAGModel(model=model)\n",
        "    model_name = model.split('/')[-1]  # Get only the part after the '/'\n",
        "    evaluation = Evaluation(name=f\"spacerag-{model_name}\", dataset=small_questions, scorers=[\n",
        "    llm_judge_scorer,\n",
        "    # ragas_score,\n",
        "    tonic_validate_score\n",
        "])\n",
        "    print(f\"RAG Model: {model}\")\n",
        "    await evaluation.evaluate(rag_model, __weave={\"display_name\": f\"spacerag-{model_name}\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLGtl-vM7yLu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "name": "SpaceRAG-initial.ipynb",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}